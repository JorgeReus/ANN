{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiperparámetros\n",
    "## Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemas\n",
    "- Los errores se suman en cada una de las capas y eso provoca que se haga overflow, por eso se necesita el **learning rate $\\eta$**\n",
    "- Converge en Mínimos locales y ni es seguro que converja en el global, por ende **SIEMPRE GUARDAR**\n",
    "### Modificaciones al gradiente\n",
    "- $ \\eta $\n",
    "- Momentum: Inercia en cuestión de la posición inicial y la pendiente de los mínimos locales, es una acumulaciónd de los pasos anteriores y tiene una atenuador **$\\alpha$** (hay varias versiones de este)\n",
    "- AdaGrad (Gradiente Adaptativo): Version modificada del GED, el cual modifica la velocidad de aprendizaje en cada paso para $\\eta$\n",
    "- AdaDelta: Es una modificación del adaGrad que modifica el momentum con respecto a una ventana de tiempo\n",
    "- Adam: Estimador de momento adaptativo, es otro método que adapta la velocidad de aprendizaje, **Es el método más completo**\n",
    "- El valor de $\\eta$ recomendado es 0.001\n",
    "- ignorar el momentum por ahora\n",
    "### Tamaño de Batch\n",
    "#### Mini batch (stochastic) gradient descent\n",
    "- Su tamaño depende de lo que tengamos de RAM (1 - RAM)\n",
    "- Es el primo hermano del learning rate\n",
    "- O movemos el lote o el laerning rate pero no ambos al mismo tiempo\n",
    "- El tamaño de lotes en la mayoria de las veces son mejores que el learning rate\n",
    "- Los lotes generalizan conocimiento\n",
    "\n",
    "## Funciones de activación\n",
    "### Sigmoidales\n",
    "#### Problemas\n",
    "- No son ocupadas mucho en las capas iniciales por que los errores son muy grandes en estas, por lo tanto el ajuste es muy pequeño\n",
    "- Lo anterior se invalida si tenemos normalización entre capa y capa\n",
    "### RELU\n",
    "- **Jamás poner purelines entre capa y capa siempre poner funciones no lineales**\n",
    "- Se usa por que tiene una transformada no lineal y no se satura, ayuda a que converjan bien los gradientes\n",
    "#### Problemas\n",
    "- Tiene una zona muerta en - $\\alpha$ y 0 $\\alpha > 0$\n",
    "#### Versiones\n",
    "##### Softplus\n",
    "- No tiene zona muerta\n",
    "##### Leaky ELU\n",
    "- Nos permiten trabajan en zona muerta de la RELU\n",
    "##### ELU\n",
    "- También nos permite trabajar en la zona muerta de la RELU\n",
    "### Funciones de capa de salidas\n",
    "#### Softmax\n",
    "- Nos permite ubicar fenónemos \"no sé\"\n",
    "- Se usa para clasificación, permitiendo determinar un invervalo de confianza y nos permite decir si le creemos o no a la red"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANN",
   "language": "python",
   "name": "ann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
